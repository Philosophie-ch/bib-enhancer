# Crossref Flow Analysis

Understanding how Crossref processes bibliographic data from API to CSV output.

## Flow Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CLI LAYER (Imperative Shell)                â”‚
â”‚              crossref_journal_scraping_cli.py                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Parse CLI args (ISSN, year range, bibkey matching config)  â”‚
â”‚  2. Load env vars (CROSSREF_EMAIL)                             â”‚
â”‚  3. Setup infrastructure                                        â”‚
â”‚  4. Wire concrete implementations                               â”‚
â”‚  5. Call orchestrator                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ORCHESTRATION LAYER (Abstract)                 â”‚
â”‚                   ports/journal_scraping.py                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  main(main_in):                                                 â”‚
â”‚    1. Call get_journal_articles() â†’ Generator[ParsedResult]    â”‚
â”‚    2. Optionally call match_bibkey() on each result            â”‚
â”‚    3. Call write_articles() to output                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    GATEWAY LAYER (Adapter)                      â”‚
â”‚            crossref_bibitem_gateway.py                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  get_journal_articles(config, journal_scraper_in):             â”‚
â”‚    1. Fetch raw data from Crossref API                         â”‚
â”‚    2. Convert each raw response â†’ ParsedResult[BibItem]        â”‚
â”‚    3. Return generator of ParsedResult[BibItem]                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONVERTER LAYER                              â”‚
â”‚              crossref_converter.py                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  convert_crossref_response_to_bibitem(raw_object):             â”‚
â”‚    1. Parse raw dict â†’ CrossrefArticle (Pydantic)              â”‚
â”‚    2. Convert CrossrefArticle â†’ BibItem                        â”‚
â”‚    3. Return ParsedResult[BibItem] (success or error)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DOMAIN LAYER (Optional)                       â”‚
â”‚              domain/bibkey_matching.py                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  match_bibkey_to_article(index, parsed_result):                â”‚
â”‚    - If parsing succeeded, match bibkey from index             â”‚
â”‚    - Update BibItem with bibkey                                â”‚
â”‚    - Return ParsedResult[BibItem]                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     OUTPUT LAYER                                â”‚
â”‚            write_articles_to_csv()                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Format each BibItem using format_bibitem()                 â”‚
â”‚  2. Add parsing_status, message, context columns               â”‚
â”‚  3. Write to CSV file                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Data Transformations

### Step 1: Raw Crossref Response â†’ CrossrefArticle

**Input:** `Dict[Any, Any]` from Crossref API
```python
{
    "DOI": "10.1234/example",
    "title": ["Article Title"],
    "author": [{"given": "John", "family": "Doe"}],
    "published": {"date-parts": [[2024]]},
    ...
}
```

**Output:** `CrossrefArticle` (Pydantic model)
```python
CrossrefArticle(
    DOI="10.1234/example",
    title=["Article Title"],
    author=[CrossrefAuthor(given="John", family="Doe")],
    issued=CrossrefDateParts(date_parts=[[2024]]),
    ...
)
```

### Step 2: CrossrefArticle â†’ BibItem

**Input:** `CrossrefArticle`

**Output:** `ParsedResult[BibItem]`
```python
{
    "parsing_status": "success",
    "out": BibItem(
        title=BibString(latex="Article Title"),
        author=(Author(...), ),
        date={"year": 2024},
        ...
    )
}
```

OR (on error):
```python
{
    "parsing_status": "error",
    "message": "Failed to convert: ...",
    "context": "raw data..."
}
```

### Step 3: BibItem â†’ BibItem with Bibkey (Optional)

**Input:** `ParsedResult[BibItem]` (no bibkey)

**Process:** Match against ODS index by journal + volume + number

**Output:** `ParsedResult[BibItem]` (with bibkey if found)

### Step 4: ParsedResult[BibItem] â†’ CSV Row

**Input:** `ParsedResult[BibItem]`

**Output:** CSV row with columns:
```
bibkey, title, author, year, journal, volume, number, pages, doi, url,
parsing_status, message, context, ...
```

## Key Abstractions

### 1. Gateway Functions

```python
def get_journal_articles(
    config: CrossrefGatewayConfig,
    main_in: JournalScraperIN,
) -> TJournalScraperOUT:
    """
    Returns: Generator[ParsedResult[BibItem], None, None]
    """
```

- Takes config + input params
- Returns **generator** of `ParsedResult[BibItem]`
- Lazy evaluation (doesn't fetch all at once)

### 2. ParsedResult Type

```python
type ParsedResult[T] = ParsingSuccess[T] | ParsingError

# Success:
{
    "parsing_status": "success",
    "out": T  # The actual BibItem
}

# Error:
{
    "parsing_status": "error",
    "message": str,  # Error description
    "context": str   # Raw data for debugging
}
```

### 3. Dependency Injection

All concrete implementations injected via `JournalScraperMainIN`:

```python
JournalScraperMainIN(
    journal_scraper_in=...,           # Input params
    get_journal_articles=...,         # Gateway function
    match_bibkey=...,                 # Optional bibkey matcher
    write_articles=...,               # CSV writer
    output_dir=...                    # Output location
)
```

## Configuration Pattern

### Gateway Config (NamedTuple)

```python
class CrossrefGatewayConfig(NamedTuple):
    client: CrossrefClient
```

### Configure Function (Partial Application)

```python
def configure(config: CrossrefGatewayConfig) -> SimpleNamespace:
    """Bind all gateway functions to config."""
    # Returns namespace with partially applied functions
    # e.g., gateway.get_journal_articles(journal_scraper_in)
```

Usage:
```python
config = CrossrefGatewayConfig(client=CrossrefClient(...))
gateway = crossref_bibitem_gateway.configure(config)
gateway.get_journal_articles(...)  # config already bound
```

## Error Handling Strategy

1. **At conversion level:** Try/catch â†’ return `ParsingError`
2. **At orchestration level:** Accept both success and error results
3. **At output level:** Write both successful and failed items to CSV with status

This means:
- **No exceptions bubble up** from data processing
- **All items tracked** (success or failure)
- **Easy debugging** via context field

## For RawText Gateway

We need to mirror this pattern:

1. **Gateway function** that returns `Generator[ParsedResult[BibItem], None, None]`
2. **Input model** that specifies what to scrape (e.g., list of URLs)
3. **Converter** from `RawTextBibitem â†’ BibItem` (already done!)
4. **CSV writer** (can reuse Crossref's `write_articles_to_csv`)
5. **CLI** that wires everything together
6. **Optionally:** Bibkey matching (can reuse existing)

### Key Differences for RawText:

- **Input:** List of URLs instead of ISSN + year range
- **Fetching:** Web scraping instead of API calls
- **Parsing:** LLM step instead of structured JSON
- **Output:** Same CSV format (reuse!)

### What We Already Have:

âœ… `RawTextBibitem` model (intermediate)
âœ… `convert_raw_text_to_bibitem()` converter
âœ… `get_bibitem_from_url()` gateway function

### What We Need:

ðŸ”² Gateway function that accepts **multiple URLs** and returns generator
ðŸ”² CLI for batch URL processing
ðŸ”² Integration with existing orchestration layer
ðŸ”² (Optional) Claude Code slash command for manual LLM step
